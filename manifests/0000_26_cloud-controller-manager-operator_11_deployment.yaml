apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-cloud-controller-manager-operator
  namespace: openshift-cloud-controller-manager-operator
  annotations:
    capability.openshift.io/name: CloudControllerManager
    include.release.openshift.io/self-managed-high-availability: "true"
    include.release.openshift.io/single-node-developer: "true"
  labels:
    k8s-app: cloud-manager-operator
spec:
  selector:
    matchLabels:
      k8s-app: cloud-manager-operator
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      annotations:
        target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
        kubectl.kubernetes.io/default-container: cluster-cloud-controller-manager
      labels:
        k8s-app: cloud-manager-operator
    spec:
      priorityClassName: system-node-critical
      serviceAccountName: cluster-cloud-controller-manager
      containers:
      - name: cluster-cloud-controller-manager
        image: quay.io/openshift/origin-cluster-cloud-controller-manager-operator
        command:
        - /bin/bash
        - -c
        - |
          #!/bin/bash
          set -o allexport
          if [[ -f /etc/kubernetes/apiserver-url.env ]]; then
            source /etc/kubernetes/apiserver-url.env
          else
            URL_ONLY_KUBECONFIG=/etc/kubernetes/kubeconfig
          fi
          exec /cluster-controller-manager-operator \
          --leader-elect=true \
          --leader-elect-lease-duration=137s \
          --leader-elect-renew-deadline=107s \
          --leader-elect-retry-period=26s \
          --leader-elect-resource-namespace=openshift-cloud-controller-manager-operator \
          "--images-json=/etc/cloud-controller-manager-config/images.json" \
          --metrics-bind-address=127.0.0.1:9257 \
          --health-addr=127.0.0.1:9259
        ports:
        - containerPort: 9257
          name: metrics
          protocol: TCP
        - containerPort: 9259
          name: healthz
          protocol: TCP
        env:
        - name: RELEASE_VERSION
          value: "0.0.1-snapshot"
        resources:
          requests:
            cpu: 10m
            memory: 50Mi
        terminationMessagePolicy: FallbackToLogsOnError
        volumeMounts:
        - name: images
          mountPath: /etc/cloud-controller-manager-config/
        - mountPath: /etc/kubernetes
          name: host-etc-kube
          readOnly: true
      - name: config-sync-controllers
        image: quay.io/openshift/origin-cluster-cloud-controller-manager-operator
        command:
          - /bin/bash
          - -c
          - |
            #!/bin/bash
            set -o allexport
            if [[ -f /etc/kubernetes/apiserver-url.env ]]; then
              source /etc/kubernetes/apiserver-url.env
            else
              URL_ONLY_KUBECONFIG=/etc/kubernetes/kubeconfig
            fi
            exec /config-sync-controllers \
            --leader-elect=true \
            --leader-elect-lease-duration=137s \
            --leader-elect-renew-deadline=107s \
            --leader-elect-retry-period=26s \
            --leader-elect-resource-namespace=openshift-cloud-controller-manager-operator \
            --health-addr=127.0.0.1:9260
        ports:
        - containerPort: 9260
          name: healthz
          protocol: TCP
        env:
        - name: RELEASE_VERSION
          value: "0.0.1-snapshot"
        resources:
          requests:
            cpu: 10m
            memory: 25Mi
        terminationMessagePolicy: FallbackToLogsOnError
        volumeMounts:
          - mountPath: /etc/kubernetes
            name: host-etc-kube
            readOnly: true
      - args:
        - --secure-listen-address=0.0.0.0:9258
        - --upstream=http://127.0.0.1:9257/
        - --tls-cert-file=/etc/tls/private/tls.crt
        - --tls-private-key-file=/etc/tls/private/tls.key
        - --config-file=/etc/kube-rbac-proxy/config-file.yaml
        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305
        - --logtostderr=true
        - --v=3
        image: placeholder.url.oc.will.replace.this.org/placeholdernamespace:kube-rbac-proxy
        imagePullPolicy: IfNotPresent
        name: kube-rbac-proxy
        ports:
        - containerPort: 9258
          name: https
          protocol: TCP
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        resources:
          requests:
            memory: 20Mi
            cpu: 10m
        terminationMessagePolicy: FallbackToLogsOnError
        volumeMounts:
        - mountPath: /etc/kube-rbac-proxy
          name: auth-proxy-config
        - mountPath: /etc/tls/private
          name: cloud-controller-manager-operator-tls
      hostNetwork: true
      nodeSelector:
        node-role.kubernetes.io/master: ""
      restartPolicy: Always
      tolerations:
      - key: node.cloudprovider.kubernetes.io/uninitialized
        effect: NoSchedule
        value: "true"
      - key: "node-role.kubernetes.io/master"
        operator: "Exists"
        effect: "NoSchedule"
      - key: "node.kubernetes.io/unreachable"
        operator: "Exists"
        effect: "NoExecute"
        tolerationSeconds: 120
      - key: "node.kubernetes.io/not-ready"
        operator: "Exists"
        effect: "NoExecute"
        tolerationSeconds: 120
      - key: "node.cloudprovider.kubernetes.io/uninitialized"
        operator: "Exists"
        effect: "NoSchedule"
        # CNI relies on CCM to fill in IP information on Node objects.
        # Therefore we must schedule before the CNI can mark the Node as ready.
      - key: "node.kubernetes.io/not-ready"
        operator: "Exists"
        effect: "NoSchedule"
      volumes:
      - name: images
        configMap:
          defaultMode: 420
          name: cloud-controller-manager-images
      - name: host-etc-kube
        hostPath:
          path: /etc/kubernetes
          type: Directory
      - configMap:
          name: kube-rbac-proxy
        name: auth-proxy-config
      - name: cloud-controller-manager-operator-tls
        secret:
          secretName: cloud-controller-manager-operator-tls
          optional: true
